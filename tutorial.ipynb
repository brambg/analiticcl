{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130694eb",
   "metadata": {},
   "source": [
    "# Analiticcl tutorial (using Python)\n",
    "\n",
    "Analiticcl is an approximate string matching or fuzzy-matching system that can be used for spelling\n",
    "correction or text normalisation (such as post-OCR correction or post-HTR correction). Texts can be checked against a\n",
    "validated or corpus-derived lexicon (with or without frequency information) and spelling variants will be returned.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Analiticcl can be invoked from either the command-line or via Python using the binding binding.\n",
    "In this tutorial, we will use the latter option and explore some of the functionality of analiticcl.\n",
    "\n",
    "First of all, we need to install analiticcl, in a Jupyter Notebook this is simply accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install analiticcl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c407b",
   "metadata": {},
   "source": [
    "When invoked from the command line instead, do the following to create a Python virtual environment and install analiticcl in it:\n",
    "\n",
    "```\n",
    "$ python -m venv env\n",
    "$ . env/bin/activate\n",
    "$ pip install analiticcl\n",
    "```\n",
    "\n",
    "Now analiticcl is installed, we can import the module. As we usually only need three main classes, we import only these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44323995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analiticcl import VariantModel, Weights, SearchParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191592cc",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Analiticcl doesn't do much out-of-the-box and is only as good as the data you feed it. It specifically needs *lexicons* or *variant lists* to operate, these contain the words or phrases that the system will match against.\n",
    "\n",
    "**Advanced note:** All input for analiticcl must be UTF-8 encoded and use unix-style line endings, NFC unicode normalisation is strongly recommended.\n",
    "\n",
    "### Alphabet file\n",
    "\n",
    "We first of all need an *alphabet file* which simply defines all characters in the alphabet, grouping certain character variants together if desired. See the [README.md](README.md) for further documentation on this. We simply take the example alphabet file that is supplied with analiticcl. The alphabet file is a TSV file (tab separated fields) containing all characters of the alphabet. Each line describes a\n",
    "single alphabet 'character', all columns on the same line are considered equivalent variants of the same character from the perspective of analiticcl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3b4109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\tA\tá\tà\tÁ\tÀ\tä\tÄ\tã\tÃ\tâ\tÂ\n",
      "e\tE\të\té\tè\tê\tË\tÉ\tÈ\tÊ\tæ\tÆ\n",
      "o\tO\tö\tó\tò\tõ\tô\tÖ\tÓ\tÒ\tÕ\tÔ\tå\tÅ\tø\tœ\u001b\n",
      "i\tI\tï\tí\tÍ\n",
      "u\tU\tú\tÚ\tü\tÜ\n",
      "y\tY\n",
      "b\tB\n",
      "c\tC\n",
      "d\tD\n",
      "f\tf\n",
      "g\tG\n",
      "h\tH\n",
      "k\tk\n",
      "l\tL\n",
      "m\tM\n",
      "n\tN\tñ\tÑ\n",
      "p\tP\n",
      "r\tR\n",
      "s\tS\n",
      "t\tT\n",
      "j\tJ\n",
      "v\tV\n",
      "w\tW\n",
      "q\tQ\n",
      "x\tX\n",
      "z\tZ\n",
      "\"\t``\t''\n",
      "'\n",
      "\\s\t\\t\n",
      ".\t,\t:\t?\t!\n",
      "0\t1\t2\t3\t4\t5\t6\t7\t8\t9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphabet_file = \"examples/simple.alphabet.tsv\"\n",
    "\n",
    "with open(alphabet_file,'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9664e0",
   "metadata": {},
   "source": [
    "### Lexicon\n",
    "\n",
    "In this tutorial we will use an English lexicon from the [GNU aspell](http://aspell.net/) project, a commonly used spell checker library. It simply contains one word per line. An example is supplied with analiticcl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_file = \"examples/eng.aspell.lexicon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589888ae",
   "metadata": {},
   "source": [
    "## Variant Model\n",
    "\n",
    "We now have all we need to build our first variant model using Analiticcl.  A variant model enables quickly and efficiently matching any input to specified lexicons, effectively matching the input text against the lexicons and in doing so finding variants of the input (or variants of the lexicon entries, it's only a matter of perspective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariantModel(alphabet_file, Weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30211f8c",
   "metadata": {},
   "source": [
    "For the time being we're content with the default weights (more about these later), passed as second parameter.\n",
    "\n",
    "The model is still empty upon instantiation. We need to feed it with one or more lexicons. Let's pass the English aspell lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969492eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.read_lexicon(lexicon_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4932e",
   "metadata": {},
   "source": [
    "Now the model is loaded we can query it as follows, let's take an existing word that's in the model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2873e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants  = model.find_variants(\"separate\")\n",
    "print(variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b4905",
   "metadata": {},
   "source": [
    "And let's now try it with misspelled input that is not in the actual model, we expect the properly spelled variant to come out on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cacf9e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "variants = model.find_variants(\"seperate\")\n",
    "print(variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3985f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
