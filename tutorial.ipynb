{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130694eb",
   "metadata": {},
   "source": [
    "# Analiticcl tutorial (using Python)\n",
    "\n",
    "Analiticcl is an approximate string matching or fuzzy-matching system that can be used for spelling\n",
    "correction or text normalisation (such as post-OCR correction or post-HTR correction). Texts can be checked against a\n",
    "validated or corpus-derived lexicon (with or without frequency information) and spelling variants will be returned.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Analiticcl can be invoked from either the command-line or via Python using the binding binding.\n",
    "In this tutorial, we will use the latter option and explore some of the functionality of analiticcl.\n",
    "\n",
    "First of all, we need to install analiticcl, in a Jupyter Notebook this is simply accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa12b213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting analiticcl\n",
      "  Downloading analiticcl-0.4.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: analiticcl\n",
      "Successfully installed analiticcl-0.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install analiticcl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c407b",
   "metadata": {},
   "source": [
    "When invoked from the command line instead, do the following to create a Python virtual environment and install analiticcl in it:\n",
    "\n",
    "```\n",
    "$ python -m venv env\n",
    "$ . env/bin/activate\n",
    "$ pip install analiticcl\n",
    "```\n",
    "\n",
    "Now analiticcl is installed, we can import the module. As we usually only need three main classes, we import only these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44323995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analiticcl import VariantModel, Weights, SearchParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191592cc",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Analiticcl doesn't do much out-of-the-box and is only as good as the data you feed it. It specifically needs *lexicons* or *variant lists* to operate, these contain the words or phrases that the system will match against.\n",
    "\n",
    "**Advanced note:** All input for analiticcl must be UTF-8 encoded and use unix-style line endings, NFC unicode normalisation is strongly recommended.\n",
    "\n",
    "### Alphabet file\n",
    "\n",
    "We first of all need an *alphabet file* which simply defines all characters in the alphabet, grouping certain character variants together if desired. See the [README.md](README.md) for further documentation on this. We simply take the example alphabet file that is supplied with analiticcl. The alphabet file is a TSV file (tab separated fields) containing all characters of the alphabet. Each line describes a\n",
    "single alphabet 'character', all columns on the same line are considered equivalent variants of the same character from the perspective of analiticcl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3b4109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\tA\tá\tà\tÁ\tÀ\tä\tÄ\tã\tÃ\tâ\tÂ\n",
      "e\tE\të\té\tè\tê\tË\tÉ\tÈ\tÊ\tæ\tÆ\n",
      "o\tO\tö\tó\tò\tõ\tô\tÖ\tÓ\tÒ\tÕ\tÔ\tå\tÅ\tø\tœ\u001b\n",
      "i\tI\tï\tí\tÍ\n",
      "u\tU\tú\tÚ\tü\tÜ\n",
      "y\tY\n",
      "b\tB\n",
      "c\tC\n",
      "d\tD\n",
      "f\tf\n",
      "g\tG\n",
      "h\tH\n",
      "k\tk\n",
      "l\tL\n",
      "m\tM\n",
      "n\tN\tñ\tÑ\n",
      "p\tP\n",
      "r\tR\n",
      "s\tS\n",
      "t\tT\n",
      "j\tJ\n",
      "v\tV\n",
      "w\tW\n",
      "q\tQ\n",
      "x\tX\n",
      "z\tZ\n",
      "\"\t``\t''\n",
      "'\n",
      "\\s\t\\t\n",
      ".\t,\t:\t?\t!\n",
      "0\t1\t2\t3\t4\t5\t6\t7\t8\t9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alphabet_file = \"examples/simple.alphabet.tsv\"\n",
    "\n",
    "with open(alphabet_file,'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9664e0",
   "metadata": {},
   "source": [
    "### Lexicon\n",
    "\n",
    "In this tutorial we will use an English lexicon from the [GNU aspell](http://aspell.net/) project, a commonly used spell checker library. It simply contains one word per line. An example is supplied with analiticcl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281a84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_file = \"examples/eng.aspell.lexicon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589888ae",
   "metadata": {},
   "source": [
    "## Variant Model\n",
    "\n",
    "### Building\n",
    "\n",
    "We now have all we need to build our first variant model using Analiticcl.  A variant model enables quickly and efficiently matching any input to specified lexicons, effectively matching the input text against the lexicons and in doing so finding variants of the input (or variants of the lexicon entries, it's only a matter of perspective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ff266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariantModel(alphabet_file, Weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30211f8c",
   "metadata": {},
   "source": [
    "For the time being we're content with the default weights (more about these later), passed as second parameter.\n",
    "\n",
    "The model is still empty upon instantiation. We need to feed it with one or more lexicons. Let's pass the English aspell lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "969492eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.read_lexicon(lexicon_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cc3e3",
   "metadata": {},
   "source": [
    "After loading all lexicon, we build the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9047d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing anagram values for all items in the lexicon...\n",
      " - Found 119773 instances\n",
      "Adding all instances to the index...\n",
      " - Found 108802 anagrams\n",
      "Creating sorted secondary index...\n",
      "Sorting secondary index...\n",
      " - Found 27 anagrams of length 1\n",
      " - Found 248 anagrams of length 2\n",
      " - Found 942 anagrams of length 3\n",
      " - Found 2593 anagrams of length 4\n",
      " - Found 5623 anagrams of length 5\n",
      " - Found 10163 anagrams of length 6\n",
      " - Found 14617 anagrams of length 7\n",
      " - Found 16911 anagrams of length 8\n",
      " - Found 16391 anagrams of length 9\n",
      " - Found 13930 anagrams of length 10\n",
      " - Found 10650 anagrams of length 11\n",
      " - Found 7194 anagrams of length 12\n",
      " - Found 4434 anagrams of length 13\n",
      " - Found 2459 anagrams of length 14\n",
      " - Found 1384 anagrams of length 15\n",
      " - Found 667 anagrams of length 16\n",
      " - Found 339 anagrams of length 17\n",
      " - Found 128 anagrams of length 18\n",
      " - Found 62 anagrams of length 19\n",
      " - Found 20 anagrams of length 20\n",
      " - Found 9 anagrams of length 21\n",
      " - Found 8 anagrams of length 22\n",
      " - Found 2 anagrams of length 23\n",
      " - Found 1 anagrams of length 24\n",
      "Constructing Language Model...\n",
      " - No language model provided\n"
     ]
    }
   ],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4932e",
   "metadata": {},
   "source": [
    "### Querying\n",
    "\n",
    "Now the model is loaded we can query it as follows, let's take an existing word that's in the model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2873e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'separate', 'score': 1.0, 'dist_score': 1.0, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'separated', 'score': 0.8125, 'dist_score': 0.8125, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'separates', 'score': 0.8125, 'dist_score': 0.8125, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'separately', 'score': 0.75, 'dist_score': 0.75, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': \"separate's\", 'score': 0.75, 'dist_score': 0.75, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'separative', 'score': 0.734375, 'dist_score': 0.734375, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'separator', 'score': 0.71875, 'dist_score': 0.71875, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'separable', 'score': 0.703125, 'dist_score': 0.703125, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'serrate', 'score': 0.65625, 'dist_score': 0.65625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'disparate', 'score': 0.625, 'dist_score': 0.625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'aerate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'prate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'operate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'aspirate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'desperate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'spate', 'score': 0.546875, 'dist_score': 0.546875, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'spare', 'score': 0.515625, 'dist_score': 0.515625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'serape', 'score': 0.515625, 'dist_score': 0.515625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'seawater', 'score': 0.515625, 'dist_score': 0.515625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n"
     ]
    }
   ],
   "source": [
    "variants  = model.find_variants(\"separate\", SearchParameters())\n",
    "for variant in variants:\n",
    "    print(variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c5812",
   "metadata": {},
   "source": [
    "As expected, the word itself is returned with a perfect score of *1.0*, along with various lower-ranking variants. Each variant is represented as a dictionary with the following keys:\n",
    "\n",
    "* ``text`` - The textual value (str) of the variant as it occurs in the lexicon\n",
    "* ``score`` - The combined score of this variant (float), a weighted combination of `dist_score` and `freq_score`\n",
    "* ``dist_score`` - The distance score (float). A perfect match always has score *1.0*.\n",
    "* ``freq_score`` - The frequency score (float), in case lexicons have frequency information. The most frequent match always has score *1.0*.\n",
    "* ``lexicons`` - The lexicons that were matched (list).\n",
    "\n",
    "And let's now try it with misspelled input that is not in the actual model, even though it's not an exact match, we expect the properly spelled variant to come out on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45cacf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'separate', 'score': 0.734375, 'dist_score': 0.734375, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'operate', 'score': 0.6875, 'dist_score': 0.6875, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'desperate', 'score': 0.6875, 'dist_score': 0.6875, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'temperate', 'score': 0.6875, 'dist_score': 0.6875, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'serrate', 'score': 0.65625, 'dist_score': 0.65625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'exasperate', 'score': 0.625, 'dist_score': 0.625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'separated', 'score': 0.609375, 'dist_score': 0.609375, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'separates', 'score': 0.609375, 'dist_score': 0.609375, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'prate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'desecrate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'segregate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'penetrate', 'score': 0.5625, 'dist_score': 0.5625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'spate', 'score': 0.546875, 'dist_score': 0.546875, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'serape', 'score': 0.53125, 'dist_score': 0.53125, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'secrete', 'score': 0.53125, 'dist_score': 0.53125, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'operates', 'score': 0.53125, 'dist_score': 0.53125, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'deprecate', 'score': 0.53125, 'dist_score': 0.53125, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n",
      "{'text': 'seepage', 'score': 0.515625, 'dist_score': 0.515625, 'freq_score': 1.0, 'lexicons': ['examples/eng.aspell.lexicon']}\n"
     ]
    }
   ],
   "source": [
    "variants = model.find_variants(\"seperate\", SearchParameters())\n",
    "for variant in variants:\n",
    "    print(variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3985f",
   "metadata": {},
   "source": [
    "The `find_variants` method is used to *query* the model directly. Parameters can be specified as part of ``SearchParameters`` using keyword arguments, the following are supported:\n",
    "\n",
    "* ``max_edit_distance`` - Maximum edit distance (levenshtein-damarau). Insertions, deletions, substitutions and transposition all have the same cost (1). It is recommended to set this value slightly lower than the maximum anagram distance. This may take an absolute integer value, i.e. the difference in characters (regardless of order), a floating point value in the range 0-1 to express a relative is expressed ratio of the total length of the text fragment under consideration, or a tuple of a floating point value and an integer (same interpretation as above) with the integer acting as a limit.\n",
    "* ``max_anagram_distance`` - Maximum anagram distance (e heuristic approximation of edit distance). This may take an absolute integer value, i.e. the difference in characters (regardless of order), a floating point value in the range 0-1 to express a relative is expressed ratio of the total length of the text fragment under consideration, or a tuple of a floating point value and an integer (same interpretation as above) with the integer acting as a limit.\n",
    "* ``score_threshold`` - Require scores to meet this threshold (float), they are pruned otherwise\n",
    "* ``cutoff_threshold`` - Cut-off threshold: if a score in the ranking is a specific factor greater than the best score, the ranking will be cut-off at that point and the score not included. Should be set to a value like 2.\n",
    "* ``freq_weight`` - Weight attributed to the frequency information in frequency reranking, in relation to the similarity (distance) component. 0 = disabled)\n",
    "* ``max_matches`` - Number of matches to return per input (set to 0 for unlimited if you want to exhaustively return every possibility within the specified anagram and edit distance).\n",
    "\n",
    "### Searching\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
